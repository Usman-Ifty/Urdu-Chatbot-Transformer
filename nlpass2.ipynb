{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13435159,"sourceType":"datasetVersion","datasetId":8527588},{"sourceId":13436775,"sourceType":"datasetVersion","datasetId":8528724}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install rouge-score sacrebleu sentencepiece -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T20:11:34.566664Z","iopub.execute_input":"2025-10-19T20:11:34.566824Z","iopub.status.idle":"2025-10-19T20:11:42.542614Z","shell.execute_reply.started":"2025-10-19T20:11:34.566809Z","shell.execute_reply":"2025-10-19T20:11:42.541777Z"}},"outputs":[{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nimport math\nfrom tqdm import tqdm\nimport sentencepiece as spm\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# Positional encoding - classic Vaswani et al. approach\nclass PositionalEncoding(nn.Module):\n    def __init__(self, dim, max_len=5000):\n        super().__init__()\n        \n        pe = torch.zeros(max_len, dim)\n        pos = torch.arange(0, max_len).unsqueeze(1).float()\n        \n        # using the formula from \"Attention is All You Need\"\n        div = torch.exp(torch.arange(0, dim, 2).float() * -(math.log(10000.0) / dim))\n        \n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        \n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n    \n    def forward(self, x):\n        # x shape: (batch, seq_len, dim)\n        return x + self.pe[:, :x.size(1)]\n\n\n# Standard scaled dot-product attention\ndef attention(q, k, v, mask=None, dropout=None):\n    d_k = q.size(-1)\n    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n    \n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)\n    \n    attn_weights = torch.softmax(scores, dim=-1)\n    \n    if dropout is not None:\n        attn_weights = dropout(attn_weights)\n    \n    output = torch.matmul(attn_weights, v)\n    return output\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, dim, n_heads, dropout_p=0.1):\n        super().__init__()\n        assert dim % n_heads == 0\n        \n        self.dim = dim\n        self.n_heads = n_heads\n        self.head_dim = dim // n_heads\n        \n        # linear projections\n        self.q_linear = nn.Linear(dim, dim)\n        self.k_linear = nn.Linear(dim, dim)\n        self.v_linear = nn.Linear(dim, dim)\n        self.out = nn.Linear(dim, dim)\n        \n        self.dropout = nn.Dropout(dropout_p)\n    \n    def forward(self, q, k, v, mask=None):\n        bs = q.size(0)\n        \n        # project and split into heads\n        q = self.q_linear(q).view(bs, -1, self.n_heads, self.head_dim).transpose(1, 2)\n        k = self.k_linear(k).view(bs, -1, self.n_heads, self.head_dim).transpose(1, 2)\n        v = self.v_linear(v).view(bs, -1, self.n_heads, self.head_dim).transpose(1, 2)\n        \n        # apply attention\n        scores = attention(q, k, v, mask, self.dropout)\n        \n        # concatenate heads\n        concat = scores.transpose(1, 2).contiguous().view(bs, -1, self.dim)\n        \n        output = self.out(concat)\n        return output\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout_p=0.1):\n        super().__init__()\n        self.w1 = nn.Linear(dim, hidden_dim)\n        self.w2 = nn.Linear(hidden_dim, dim)\n        self.dropout = nn.Dropout(dropout_p)\n    \n    def forward(self, x):\n        return self.w2(self.dropout(torch.relu(self.w1(x))))\n\n\nclass EncoderBlock(nn.Module):\n    def __init__(self, dim, n_heads, ff_dim, dropout_p=0.1):\n        super().__init__()\n        self.attn = MultiHeadAttention(dim, n_heads, dropout_p)\n        self.ff = FeedForward(dim, ff_dim, dropout_p)\n        self.norm1 = nn.LayerNorm(dim)\n        self.norm2 = nn.LayerNorm(dim)\n        self.dropout1 = nn.Dropout(dropout_p)\n        self.dropout2 = nn.Dropout(dropout_p)\n    \n    def forward(self, x, mask):\n        # self-attention with residual\n        attn_out = self.attn(x, x, x, mask)\n        x = self.norm1(x + self.dropout1(attn_out))\n        \n        # feed-forward with residual\n        ff_out = self.ff(x)\n        x = self.norm2(x + self.dropout2(ff_out))\n        \n        return x\n\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, dim, n_heads, ff_dim, dropout_p=0.1):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(dim, n_heads, dropout_p)\n        self.cross_attn = MultiHeadAttention(dim, n_heads, dropout_p)\n        self.ff = FeedForward(dim, ff_dim, dropout_p)\n        \n        self.norm1 = nn.LayerNorm(dim)\n        self.norm2 = nn.LayerNorm(dim)\n        self.norm3 = nn.LayerNorm(dim)\n        \n        self.dropout1 = nn.Dropout(dropout_p)\n        self.dropout2 = nn.Dropout(dropout_p)\n        self.dropout3 = nn.Dropout(dropout_p)\n    \n    def forward(self, x, enc_out, src_mask, tgt_mask):\n        # masked self-attention\n        self_attn_out = self.self_attn(x, x, x, tgt_mask)\n        x = self.norm1(x + self.dropout1(self_attn_out))\n        \n        # cross-attention to encoder\n        cross_attn_out = self.cross_attn(x, enc_out, enc_out, src_mask)\n        x = self.norm2(x + self.dropout2(cross_attn_out))\n        \n        # feed-forward\n        ff_out = self.ff(x)\n        x = self.norm3(x + self.dropout3(ff_out))\n        \n        return x\n\n\nclass Transformer(nn.Module):\n    def __init__(self, vocab_size, dim=128, n_heads=4, n_enc_layers=2, \n                 n_dec_layers=2, ff_dim=512, max_len=512, dropout_p=0.1):\n        super().__init__()\n        \n        self.dim = dim\n        self.vocab_size = vocab_size\n        \n        # embeddings\n        self.src_embed = nn.Embedding(vocab_size, dim)\n        self.tgt_embed = nn.Embedding(vocab_size, dim)\n        self.pos_enc = PositionalEncoding(dim, max_len)\n        \n        # encoder stack\n        self.enc_layers = nn.ModuleList([\n            EncoderBlock(dim, n_heads, ff_dim, dropout_p) \n            for _ in range(n_enc_layers)\n        ])\n        \n        # decoder stack\n        self.dec_layers = nn.ModuleList([\n            DecoderBlock(dim, n_heads, ff_dim, dropout_p) \n            for _ in range(n_dec_layers)\n        ])\n        \n        self.dropout = nn.Dropout(dropout_p)\n        self.final_layer = nn.Linear(dim, vocab_size)\n        \n        self._reset_parameters()\n    \n    def _reset_parameters(self):\n        for p in self.parameters():\n            if p.dim() > 1:\n                nn.init.xavier_uniform_(p)\n    \n    def make_src_mask(self, src):\n        # src: (batch, src_len)\n        mask = (src != 0).unsqueeze(1).unsqueeze(2)\n        return mask\n    \n    def make_tgt_mask(self, tgt):\n        # tgt: (batch, tgt_len)\n        batch_size, tgt_len = tgt.size()\n        \n        # padding mask\n        tgt_pad_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n        \n        # causal mask (no peeking)\n        tgt_sub_mask = torch.tril(torch.ones((tgt_len, tgt_len), device=tgt.device)).bool()\n        tgt_mask = tgt_pad_mask & tgt_sub_mask\n        \n        return tgt_mask\n    \n    def encode(self, src, src_mask):\n        x = self.src_embed(src) * math.sqrt(self.dim)\n        x = self.pos_enc(x)\n        x = self.dropout(x)\n        \n        for layer in self.enc_layers:\n            x = layer(x, src_mask)\n        \n        return x\n    \n    def decode(self, tgt, enc_out, src_mask, tgt_mask):\n        x = self.tgt_embed(tgt) * math.sqrt(self.dim)\n        x = self.pos_enc(x)\n        x = self.dropout(x)\n        \n        for layer in self.dec_layers:\n            x = layer(x, enc_out, src_mask, tgt_mask)\n        \n        return x\n    \n    def forward(self, src, tgt):\n        src_mask = self.make_src_mask(src)\n        tgt_mask = self.make_tgt_mask(tgt)\n        \n        enc_out = self.encode(src, src_mask)\n        dec_out = self.decode(tgt, enc_out, src_mask, tgt_mask)\n        \n        logits = self.final_layer(dec_out)\n        return logits\n\n\n# Dataset handling\nclass QADataset(Dataset):\n    def __init__(self, questions, answers, tokenizer, max_len=128):\n        self.questions = questions\n        self.answers = answers\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.questions)\n    \n    def __getitem__(self, idx):\n        q_ids = self.tokenizer.encode_as_ids(self.questions[idx])\n        a_ids = self.tokenizer.encode_as_ids(self.answers[idx])\n        \n        # trim if too long\n        q_ids = q_ids[:self.max_len-2]\n        a_ids = a_ids[:self.max_len-2]\n        \n        # add BOS and EOS tokens (1 and 2)\n        q = torch.tensor([1] + q_ids + [2])\n        a = torch.tensor([1] + a_ids + [2])\n        \n        return q, a\n\n\ndef collate_batch(batch):\n    qs, ans = zip(*batch)\n    qs_padded = pad_sequence(qs, batch_first=True, padding_value=0)\n    ans_padded = pad_sequence(ans, batch_first=True, padding_value=0)\n    return qs_padded, ans_padded\n\n\n# Training utilities\ndef train_one_epoch(model, loader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0\n    n_batches = 0\n    \n    for src, tgt in tqdm(loader, desc=\"Training\"):\n        src, tgt = src.to(device), tgt.to(device)\n        \n        # teacher forcing\n        tgt_input = tgt[:, :-1]\n        tgt_output = tgt[:, 1:]\n        \n        optimizer.zero_grad()\n        \n        logits = model(src, tgt_input)\n        \n        loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_output.reshape(-1))\n        \n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        \n        total_loss += loss.item()\n        n_batches += 1\n    \n    return total_loss / n_batches\n\n\ndef validate_model(model, loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    n_batches = 0\n    \n    with torch.no_grad():\n        for src, tgt in tqdm(loader, desc=\"Validation\"):\n            src, tgt = src.to(device), tgt.to(device)\n            \n            tgt_input = tgt[:, :-1]\n            tgt_output = tgt[:, 1:]\n            \n            logits = model(src, tgt_input)\n            loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_output.reshape(-1))\n            \n            total_loss += loss.item()\n            n_batches += 1\n    \n    return total_loss / n_batches\n\n\n# Inference - greedy decoding\ndef generate_answer(model, question, tokenizer, device, max_len=80):\n    model.eval()\n    \n    # tokenize question\n    q_ids = tokenizer.encode_as_ids(question)\n    q_ids = [1] + q_ids[:126] + [2]  # BOS + tokens + EOS\n    src = torch.tensor([q_ids], device=device)\n    \n    # encode\n    src_mask = model.make_src_mask(src)\n    enc_out = model.encode(src, src_mask)\n    \n    # start decoding with BOS token\n    tgt_ids = [1]\n    \n    for _ in range(max_len):\n        tgt = torch.tensor([tgt_ids], device=device)\n        tgt_mask = model.make_tgt_mask(tgt)\n        \n        dec_out = model.decode(tgt, enc_out, src_mask, tgt_mask)\n        logits = model.final_layer(dec_out)\n        \n        # greedy: pick most probable token\n        next_token = logits[0, -1].argmax().item()\n        tgt_ids.append(next_token)\n        \n        # stop at EOS or max length\n        if next_token == 2:\n            break\n    \n    # decode to text\n    answer_ids = [i for i in tgt_ids if i not in [0, 1, 2]]\n    answer = tokenizer.decode_ids(answer_ids)\n    \n    return answer\n\n\n# Main training loop\ndef main():\n    # paths\n    qa_path = '/kaggle/input/nlp-a-2/qa_gk.csv'\n    tokenizer_path = '/kaggle/input/nlp-a-2/urdu_tokenizer.model'\n    \n    print(\"Loading data...\")\n    df = pd.read_csv(qa_path, encoding='utf-8')\n    \n    questions = df['Question'].dropna().astype(str).tolist()\n    answers = df['Answer'].dropna().astype(str).tolist()\n    \n    # match lengths\n    min_len = min(len(questions), len(answers))\n    questions = questions[:min_len]\n    answers = answers[:min_len]\n    \n    print(f\"Loaded {len(questions)} QA pairs\")\n    \n    # load tokenizer\n    print(\"Loading tokenizer...\")\n    tokenizer = spm.SentencePieceProcessor()\n    tokenizer.load(tokenizer_path)\n    vocab_size = len(tokenizer)\n    print(f\"Vocab size: {vocab_size}\")\n    \n    # train/val split\n    split_idx = int(len(questions) * 0.9)\n    train_q, train_a = questions[:split_idx], answers[:split_idx]\n    val_q, val_a = questions[split_idx:], answers[split_idx:]\n    \n    print(f\"Train: {len(train_q)}, Val: {len(val_q)}\")\n    \n    # datasets\n    train_ds = QADataset(train_q, train_a, tokenizer)\n    val_ds = QADataset(val_q, val_a, tokenizer)\n    \n    train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, collate_fn=collate_batch)\n    val_loader = DataLoader(val_ds, batch_size=8, shuffle=False, collate_fn=collate_batch)\n    \n    # model setup\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    model = Transformer(\n        vocab_size=vocab_size,\n        dim=128,\n        n_heads=4,\n        n_enc_layers=2,\n        n_dec_layers=2,\n        ff_dim=512,\n        dropout_p=0.2\n    ).to(device)\n    \n    n_params = sum(p.numel() for p in model.parameters())\n    print(f\"Model parameters: {n_params:,}\")\n    \n    criterion = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0.1)\n    optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-5)\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\n    \n    # training\n    best_val_loss = float('inf')\n    patience = 10\n    patience_count = 0\n    n_epochs = 100\n    \n    print(\"\\nStarting training...\")\n    \n    for epoch in range(n_epochs):\n        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n        val_loss = validate_model(model, val_loader, criterion, device)\n        scheduler.step()\n        \n        print(f\"Epoch {epoch+1:3d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\", end=\"\")\n        \n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_count = 0\n            torch.save(model.state_dict(), '/kaggle/working/urdu_chatbot.pt')\n            print(\" [saved]\")\n        else:\n            patience_count += 1\n            print()\n            \n            if patience_count >= patience:\n                print(f\"Early stopping at epoch {epoch+1}\")\n                break\n    \n    print(f\"\\nTraining finished. Best val loss: {best_val_loss:.4f}\")\n    \n    # test some questions\n    print(\"\\nTesting inference:\")\n    test_questions = [\n        \"پاکستان کا دارالحکومت کیا ہے؟\",\n        \"اسلام آباد کہاں واقع ہے؟\",\n        \"پاکستان کی کرنسی کیا ہے؟\"\n    ]\n    \n    for q in test_questions:\n        answer = generate_answer(model, q, tokenizer, device)\n        print(f\"\\nQ: {q}\")\n        print(f\"A: {answer}\")\n    \n    return model, tokenizer, device\n\n\nif __name__ == \"__main__\":\n    model, tokenizer, device = main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-19T20:11:42.544431Z","iopub.execute_input":"2025-10-19T20:11:42.544714Z","iopub.status.idle":"2025-10-19T20:16:02.403278Z","shell.execute_reply.started":"2025-10-19T20:11:42.544693Z","shell.execute_reply":"2025-10-19T20:16:02.402480Z"}},"outputs":[{"name":"stdout","text":"Loading QA dataset...\nLoaded 5816 QA pairs\nSample Q: پاکستان میں بہترین یونیورسٹیوں کا نام؟\nSample A: LUMS اور AKU پاکستان میں بہترین یونیورسٹی ہیں\n\nLoading tokenizer...\nVocab size: 16000\n\nTrain: 5234, Val: 582\n\nDevice: cuda\n\nModel parameters: 7,085,696\n\nTraining starts...\n\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 655/655 [00:14<00:00, 44.50it/s]\nValidating: 100%|██████████| 73/73 [00:00<00:00, 160.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch   1 | Train: 6.2724 | Val: 5.7475 [SAVED]\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 655/655 [00:12<00:00, 52.42it/s]\nValidating: 100%|██████████| 73/73 [00:00<00:00, 181.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch   2 | Train: 5.2041 | Val: 5.3878 [SAVED]\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 655/655 [00:12<00:00, 53.28it/s]\nValidating: 100%|██████████| 73/73 [00:00<00:00, 170.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch   3 | Train: 4.6723 | Val: 5.1254 [SAVED]\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 655/655 [00:12<00:00, 53.08it/s]\nValidating: 100%|██████████| 73/73 [00:00<00:00, 185.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch   4 | Train: 4.2604 | Val: 4.9628 [SAVED]\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 655/655 [00:12<00:00, 52.96it/s]\nValidating: 100%|██████████| 73/73 [00:00<00:00, 186.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch   5 | Train: 3.9537 | Val: 4.8656 [SAVED]\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 655/655 [00:12<00:00, 53.16it/s]\nValidating: 100%|██████████| 73/73 [00:00<00:00, 186.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch   6 | Train: 3.6956 | Val: 4.8071 [SAVED]\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 655/655 [00:12<00:00, 52.36it/s]\nValidating: 100%|██████████| 73/73 [00:00<00:00, 182.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch   7 | Train: 3.4900 | Val: 4.7904 [SAVED]\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 655/655 [00:12<00:00, 53.15it/s]\nValidating: 100%|██████████| 73/73 [00:00<00:00, 183.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch   8 | Train: 3.2946 | Val: 4.7898 [SAVED]\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 655/655 [00:12<00:00, 52.17it/s]\nValidating: 100%|██████████| 73/73 [00:00<00:00, 186.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch   9 | Train: 3.1393 | Val: 4.7709 [SAVED]\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 655/655 [00:12<00:00, 52.60it/s]\nValidating: 100%|██████████| 73/73 [00:00<00:00, 168.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch  10 | Train: 3.0106 | Val: 4.8385\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  41%|████      | 30/73 [00:01<00:01, 25.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"  BLEU: 13.22\n\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 655/655 [00:12<00:00, 52.59it/s]\nValidating: 100%|██████████| 73/73 [00:00<00:00, 186.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch  11 | Train: 2.8855 | Val: 4.8085\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 655/655 [00:12<00:00, 53.41it/s]\nValidating: 100%|██████████| 73/73 [00:00<00:00, 181.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch  12 | Train: 2.7996 | Val: 4.8527\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 655/655 [00:12<00:00, 53.56it/s]\nValidating: 100%|██████████| 73/73 [00:00<00:00, 187.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch  13 | Train: 2.7178 | Val: 4.8542\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 655/655 [00:12<00:00, 53.79it/s]\nValidating: 100%|██████████| 73/73 [00:00<00:00, 187.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch  14 | Train: 2.6510 | Val: 4.8856\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 655/655 [00:12<00:00, 52.72it/s]\nValidating: 100%|██████████| 73/73 [00:00<00:00, 135.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch  15 | Train: 2.5932 | Val: 4.8787\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 655/655 [00:12<00:00, 52.35it/s]\nValidating: 100%|██████████| 73/73 [00:00<00:00, 184.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch  16 | Train: 2.5506 | Val: 4.9054\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 655/655 [00:12<00:00, 53.21it/s]\nValidating: 100%|██████████| 73/73 [00:00<00:00, 182.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch  17 | Train: 2.5012 | Val: 4.9124\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 655/655 [00:12<00:00, 54.03it/s]\nValidating: 100%|██████████| 73/73 [00:00<00:00, 184.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch  18 | Train: 2.4654 | Val: 4.9242\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 655/655 [00:12<00:00, 52.79it/s]\nValidating: 100%|██████████| 73/73 [00:00<00:00, 188.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch  19 | Train: 2.4347 | Val: 4.9539\nEarly stopping at epoch 19\n\nTraining complete. Best loss: 4.7709\n\nTesting on sample questions:\nQ: پاکستان میں بہترین یونیورسٹیوں کا نام؟\nA: مجھے افسوس ہے کہ میں نہیں جانتا، براہ مہربانی زیادہ مخصوص رہیں \n\nQ: اسلام آباد کیا ہے؟\nA: اسلام آباد پاکستان کا دارالحکومت ہے \n\nQ: پاکستان کی کرنسی کیا ہے؟\nA: روپیہ پاکستان کی کرنسی ہے \n\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport sentencepiece as spm\nimport math\n\n\n# Same model architecture as training\nclass PositionalEncoding(nn.Module):\n    def __init__(self, dim, max_len=5000):\n        super().__init__()\n        pe = torch.zeros(max_len, dim)\n        pos = torch.arange(0, max_len).unsqueeze(1).float()\n        div = torch.exp(torch.arange(0, dim, 2).float() * -(math.log(10000.0) / dim))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n    \n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)]\n\n\ndef attention(q, k, v, mask=None, dropout=None):\n    d_k = q.size(-1)\n    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)\n    attn_weights = torch.softmax(scores, dim=-1)\n    if dropout is not None:\n        attn_weights = dropout(attn_weights)\n    return torch.matmul(attn_weights, v)\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, dim, n_heads, dropout_p=0.1):\n        super().__init__()\n        assert dim % n_heads == 0\n        self.dim = dim\n        self.n_heads = n_heads\n        self.head_dim = dim // n_heads\n        self.q_linear = nn.Linear(dim, dim)\n        self.k_linear = nn.Linear(dim, dim)\n        self.v_linear = nn.Linear(dim, dim)\n        self.out = nn.Linear(dim, dim)\n        self.dropout = nn.Dropout(dropout_p)\n    \n    def forward(self, q, k, v, mask=None):\n        bs = q.size(0)\n        q = self.q_linear(q).view(bs, -1, self.n_heads, self.head_dim).transpose(1, 2)\n        k = self.k_linear(k).view(bs, -1, self.n_heads, self.head_dim).transpose(1, 2)\n        v = self.v_linear(v).view(bs, -1, self.n_heads, self.head_dim).transpose(1, 2)\n        scores = attention(q, k, v, mask, self.dropout)\n        concat = scores.transpose(1, 2).contiguous().view(bs, -1, self.dim)\n        return self.out(concat)\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout_p=0.1):\n        super().__init__()\n        self.w1 = nn.Linear(dim, hidden_dim)\n        self.w2 = nn.Linear(hidden_dim, dim)\n        self.dropout = nn.Dropout(dropout_p)\n    \n    def forward(self, x):\n        return self.w2(self.dropout(torch.relu(self.w1(x))))\n\n\nclass EncoderBlock(nn.Module):\n    def __init__(self, dim, n_heads, ff_dim, dropout_p=0.1):\n        super().__init__()\n        self.attn = MultiHeadAttention(dim, n_heads, dropout_p)\n        self.ff = FeedForward(dim, ff_dim, dropout_p)\n        self.norm1 = nn.LayerNorm(dim)\n        self.norm2 = nn.LayerNorm(dim)\n        self.dropout1 = nn.Dropout(dropout_p)\n        self.dropout2 = nn.Dropout(dropout_p)\n    \n    def forward(self, x, mask):\n        attn_out = self.attn(x, x, x, mask)\n        x = self.norm1(x + self.dropout1(attn_out))\n        ff_out = self.ff(x)\n        x = self.norm2(x + self.dropout2(ff_out))\n        return x\n\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, dim, n_heads, ff_dim, dropout_p=0.1):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(dim, n_heads, dropout_p)\n        self.cross_attn = MultiHeadAttention(dim, n_heads, dropout_p)\n        self.ff = FeedForward(dim, ff_dim, dropout_p)\n        self.norm1 = nn.LayerNorm(dim)\n        self.norm2 = nn.LayerNorm(dim)\n        self.norm3 = nn.LayerNorm(dim)\n        self.dropout1 = nn.Dropout(dropout_p)\n        self.dropout2 = nn.Dropout(dropout_p)\n        self.dropout3 = nn.Dropout(dropout_p)\n    \n    def forward(self, x, enc_out, src_mask, tgt_mask):\n        self_attn_out = self.self_attn(x, x, x, tgt_mask)\n        x = self.norm1(x + self.dropout1(self_attn_out))\n        cross_attn_out = self.cross_attn(x, enc_out, enc_out, src_mask)\n        x = self.norm2(x + self.dropout2(cross_attn_out))\n        ff_out = self.ff(x)\n        x = self.norm3(x + self.dropout3(ff_out))\n        return x\n\n\nclass Transformer(nn.Module):\n    def __init__(self, vocab_size, dim=128, n_heads=4, n_enc_layers=2, \n                 n_dec_layers=2, ff_dim=512, max_len=512, dropout_p=0.1):\n        super().__init__()\n        self.dim = dim\n        self.vocab_size = vocab_size\n        self.src_embed = nn.Embedding(vocab_size, dim)\n        self.tgt_embed = nn.Embedding(vocab_size, dim)\n        self.pos_enc = PositionalEncoding(dim, max_len)\n        self.enc_layers = nn.ModuleList([\n            EncoderBlock(dim, n_heads, ff_dim, dropout_p) \n            for _ in range(n_enc_layers)\n        ])\n        self.dec_layers = nn.ModuleList([\n            DecoderBlock(dim, n_heads, ff_dim, dropout_p) \n            for _ in range(n_dec_layers)\n        ])\n        self.dropout = nn.Dropout(dropout_p)\n        self.final_layer = nn.Linear(dim, vocab_size)\n    \n    def make_src_mask(self, src):\n        mask = (src != 0).unsqueeze(1).unsqueeze(2)\n        return mask\n    \n    def make_tgt_mask(self, tgt):\n        batch_size, tgt_len = tgt.size()\n        tgt_pad_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n        tgt_sub_mask = torch.tril(torch.ones((tgt_len, tgt_len), device=tgt.device)).bool()\n        tgt_mask = tgt_pad_mask & tgt_sub_mask\n        return tgt_mask\n    \n    def encode(self, src, src_mask):\n        x = self.src_embed(src) * math.sqrt(self.dim)\n        x = self.pos_enc(x)\n        x = self.dropout(x)\n        for layer in self.enc_layers:\n            x = layer(x, src_mask)\n        return x\n    \n    def decode(self, tgt, enc_out, src_mask, tgt_mask):\n        x = self.tgt_embed(tgt) * math.sqrt(self.dim)\n        x = self.pos_enc(x)\n        x = self.dropout(x)\n        for layer in self.dec_layers:\n            x = layer(x, enc_out, src_mask, tgt_mask)\n        return x\n    \n    def forward(self, src, tgt):\n        src_mask = self.make_src_mask(src)\n        tgt_mask = self.make_tgt_mask(tgt)\n        enc_out = self.encode(src, src_mask)\n        dec_out = self.decode(tgt, enc_out, src_mask, tgt_mask)\n        logits = self.final_layer(dec_out)\n        return logits\n\n\nclass UrduQABot:\n    \"\"\"Easy-to-use wrapper for inference\"\"\"\n    \n    def __init__(self, model_path, tokenizer_path, device='cuda'):\n        print(\"Loading tokenizer...\")\n        self.tokenizer = spm.SentencePieceProcessor()\n        self.tokenizer.load(tokenizer_path)\n        self.vocab_size = len(self.tokenizer)\n        \n        print(\"Loading model...\")\n        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')\n        \n        self.model = Transformer(\n            vocab_size=self.vocab_size,\n            dim=128,\n            n_heads=4,\n            n_enc_layers=2,\n            n_dec_layers=2,\n            ff_dim=512,\n            dropout_p=0.2\n        ).to(self.device)\n        \n        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n        self.model.eval()\n        \n        print(f\"Model loaded on {self.device}\")\n    \n    def ask(self, question, max_len=80):\n        \"\"\"Ask a question and get an answer\"\"\"\n        self.model.eval()\n        \n        # tokenize\n        q_ids = self.tokenizer.encode_as_ids(question)\n        q_ids = [1] + q_ids[:126] + [2]\n        src = torch.tensor([q_ids], device=self.device)\n        \n        # encode\n        src_mask = self.model.make_src_mask(src)\n        enc_out = self.model.encode(src, src_mask)\n        \n        # decode\n        tgt_ids = [1]  # start with BOS\n        \n        with torch.no_grad():\n            for _ in range(max_len):\n                tgt = torch.tensor([tgt_ids], device=self.device)\n                tgt_mask = self.model.make_tgt_mask(tgt)\n                \n                dec_out = self.model.decode(tgt, enc_out, src_mask, tgt_mask)\n                logits = self.model.final_layer(dec_out)\n                \n                next_token = logits[0, -1].argmax().item()\n                tgt_ids.append(next_token)\n                \n                if next_token == 2:  # EOS\n                    break\n        \n        # decode to text\n        answer_ids = [i for i in tgt_ids if i not in [0, 1, 2]]\n        answer = self.tokenizer.decode_ids(answer_ids)\n        \n        return answer\n    \n    def chat(self):\n        \"\"\"Interactive chat mode\"\"\"\n        print(\"\\n=== Urdu QA Chatbot ===\")\n        print(\"Type 'quit' or 'exit' to stop\\n\")\n        \n        while True:\n            question = input(\"You: \").strip()\n            \n            if question.lower() in ['quit', 'exit', 'q']:\n                print(\"Goodbye!\")\n                break\n            \n            if not question:\n                continue\n            \n            answer = self.ask(question)\n            print(f\"Bot: {answer}\\n\")\n\n\n# Beam search for better quality (optional)\ndef beam_search(model, src, tokenizer, device, beam_size=3, max_len=80):\n    \"\"\"Beam search decoding for potentially better answers\"\"\"\n    model.eval()\n    \n    src_mask = model.make_src_mask(src)\n    enc_out = model.encode(src, src_mask)\n    \n    # initialize beam with BOS token\n    beams = [([1], 0.0)]  # (sequence, score)\n    \n    with torch.no_grad():\n        for step in range(max_len):\n            candidates = []\n            \n            for seq, score in beams:\n                if seq[-1] == 2:  # already finished\n                    candidates.append((seq, score))\n                    continue\n                \n                tgt = torch.tensor([seq], device=device)\n                tgt_mask = model.make_tgt_mask(tgt)\n                \n                dec_out = model.decode(tgt, enc_out, src_mask, tgt_mask)\n                logits = model.final_layer(dec_out)\n                log_probs = torch.log_softmax(logits[0, -1], dim=-1)\n                \n                # get top k tokens\n                top_probs, top_ids = torch.topk(log_probs, beam_size)\n                \n                for prob, idx in zip(top_probs, top_ids):\n                    new_seq = seq + [idx.item()]\n                    new_score = score + prob.item()\n                    candidates.append((new_seq, new_score))\n            \n            # keep top beam_size candidates\n            beams = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_size]\n            \n            # stop if all beams finished\n            if all(seq[-1] == 2 for seq, _ in beams):\n                break\n    \n    # return best sequence\n    best_seq = beams[0][0]\n    answer_ids = [i for i in best_seq if i not in [0, 1, 2]]\n    answer = tokenizer.decode_ids(answer_ids)\n    \n    return answer\n\n\ndef main():\n    # paths - update these\n    model_path = '/kaggle/working/urdu_chatbot.pt'\n    tokenizer_path = '/kaggle/input/nlp-a-2/urdu_tokenizer.model'\n    \n    # create bot\n    bot = UrduQABot(model_path, tokenizer_path)\n    \n    # test some questions\n    test_questions = [\n        \"پاکستان کا دارالحکومت کیا ہے؟\",\n        \"اسلام آباد کہاں ہے؟\",\n        \"پاکستان کی کرنسی کیا ہے؟\",\n        \"لاہور کے بارے میں بتائیں؟\"\n    ]\n    \n    print(\"\\n=== Testing Questions ===\\n\")\n    for q in test_questions:\n        ans = bot.ask(q)\n        print(f\"Q: {q}\")\n        print(f\"A: {ans}\\n\")\n    \n    # uncomment to start interactive chat\n    # bot.chat()\n\n\n# Alternative: Load and use model directly\ndef quick_inference():\n    \"\"\"Quick inference without wrapper class\"\"\"\n    \n    # setup\n    tokenizer = spm.SentencePieceProcessor()\n    tokenizer.load('/kaggle/input/nlp-a-2/urdu_tokenizer.model')\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    model = Transformer(\n        vocab_size=len(tokenizer),\n        dim=128,\n        n_heads=4,\n        n_enc_layers=2,\n        n_dec_layers=2,\n        ff_dim=512,\n        dropout_p=0.2\n    ).to(device)\n    \n    model.load_state_dict(torch.load('/kaggle/working/urdu_chatbot.pt', map_location=device))\n    model.eval()\n    \n    # ask question\n    question = \"پاکستان کے بارے میں بتائیں؟\"\n    \n    q_ids = [1] + tokenizer.encode_as_ids(question)[:126] + [2]\n    src = torch.tensor([q_ids], device=device)\n    \n    src_mask = model.make_src_mask(src)\n    enc_out = model.encode(src, src_mask)\n    \n    tgt_ids = [1]\n    \n    with torch.no_grad():\n        for _ in range(80):\n            tgt = torch.tensor([tgt_ids], device=device)\n            tgt_mask = model.make_tgt_mask(tgt)\n            dec_out = model.decode(tgt, enc_out, src_mask, tgt_mask)\n            logits = model.final_layer(dec_out)\n            next_token = logits[0, -1].argmax().item()\n            tgt_ids.append(next_token)\n            if next_token == 2:\n                break\n    \n    answer_ids = [i for i in tgt_ids if i not in [0, 1, 2]]\n    answer = tokenizer.decode_ids(answer_ids)\n    \n    print(f\"Q: {question}\")\n    print(f\"A: {answer}\")\n\n\nif __name__ == \"__main__\":\n    main()\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}